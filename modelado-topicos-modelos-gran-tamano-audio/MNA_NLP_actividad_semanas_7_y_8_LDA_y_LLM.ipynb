{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hVND8xY2OKY"
      },
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## Maestría en Inteligencia Artificial Aplicada\n",
        "#### Tecnológico de Monterrey\n",
        "#### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "### **Adtividad en Equipos Semanas 7 y 8 : LDA y LMM audio-a-texto**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aimHVFOv23lm"
      },
      "source": [
        "* **Nombres y matrículas:**\n",
        "\n",
        "  *   Eduardo Aldair Ahumada Garcia Jurado - A01422929\n",
        "  *   Elemento de lista\n",
        "  *   Elemento de lista\n",
        "\n",
        "* **Número de Equipo:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jimvsiVgjMg"
      },
      "source": [
        "* ##### **En cada ejercicio pueden importar los paquetes o librerías que requieran.**\n",
        "\n",
        "* ##### **En cada ejercicio pueden incluir las celdas y líneas de código que deseen.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BtP-Sk0DT-M"
      },
      "source": [
        "# **Ejercicio 1:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh78pKeMghfe"
      },
      "source": [
        "* #### **Liga de los audios de las fábulas de Esopo:** https://www.gutenberg.org/ebooks/21144\n",
        "\n",
        "* #### **Descargar los 10 archivos de audio solicitados: 1, 4, 5, 6, 14, 22, 24, 25, 26, 27.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Bfs5Zxc9j7Uf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already exists: 21144-01.mp3\n",
            "Already exists: 21144-04.mp3\n",
            "Already exists: 21144-05.mp3\n",
            "Already exists: 21144-06.mp3\n",
            "Already exists: 21144-14.mp3\n",
            "Already exists: 21144-22.mp3\n",
            "Already exists: 21144-24.mp3\n",
            "Already exists: 21144-25.mp3\n",
            "Already exists: 21144-26.mp3\n",
            "Already exists: 21144-27.mp3\n"
          ]
        }
      ],
      "source": [
        "# Incluyan a continuación todas las celdas (de código o texto) que deseen...\n",
        "\n",
        "# Base URL and target folder\n",
        "base_url = \"https://www.gutenberg.org/files/21144/mp3/21144-{:02d}.mp3\"\n",
        "target_folder = \"downloads\"\n",
        "file_numbers = [1, 4, 5, 6, 14, 22, 24, 25, 26, 27]\n",
        "\n",
        "# Create target folder if it doesn't exist\n",
        "os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "# Download loop\n",
        "for num in file_numbers:\n",
        "    file_url = base_url.format(num)\n",
        "    filename = f\"21144-{num:02d}.mp3\"\n",
        "    file_path = os.path.join(target_folder, filename)\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"Already exists: {filename}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Downloading: {filename}\")\n",
        "    try:\n",
        "        response = requests.get(file_url)\n",
        "        response.raise_for_status()\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download {filename}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uYgtCvvJSmq"
      },
      "source": [
        "# **Ejercicio 2a:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAQjVP2HkoZY"
      },
      "source": [
        "* #### **Comenten el por qué del modelo seleccionado para extracción del texto de los audios.**\n",
        "\n",
        "* #### **Extraer el contenido de los audios en texto.**\n",
        "\n",
        "* #### **Sugerencia:** pueden extraerlo en un formato de diccionario, clave:valor $→$ {audio01:fabula01, ...}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SCRIPT PARA USAR SOLO OPEN AI API model=\"whisper-1\"\n",
        "#### Comentado porque abajo viene un script que te permite seleccionar el modelo que quieras usar entre OPEN AI API vs modelos de huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3k5sLGhnO1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transcript found in cache: downloads/21144-01.mp3\n",
            "Transcribed (21144-01.mp3): 103 palabras\n",
            "Transcript found in cache: downloads/21144-04.mp3\n",
            "Transcribed (21144-04.mp3): 142 palabras\n",
            "Transcript found in cache: downloads/21144-05.mp3\n",
            "Transcribed (21144-05.mp3): 135 palabras\n",
            "Transcript found in cache: downloads/21144-06.mp3\n",
            "Transcribed (21144-06.mp3): 152 palabras\n",
            "Transcript found in cache: downloads/21144-14.mp3\n",
            "Transcribed (21144-14.mp3): 103 palabras\n",
            "Transcript found in cache: downloads/21144-22.mp3\n",
            "Transcribed (21144-22.mp3): 103 palabras\n",
            "Transcript found in cache: downloads/21144-24.mp3\n",
            "Transcribed (21144-24.mp3): 140 palabras\n",
            "Transcript found in cache: downloads/21144-25.mp3\n",
            "Transcribed (21144-25.mp3): 92 palabras\n",
            "Transcript found in cache: downloads/21144-26.mp3\n",
            "Transcribed (21144-26.mp3): 118 palabras\n",
            "Transcript found in cache: downloads/21144-27.mp3\n",
            "Transcribed (21144-27.mp3): 95 palabras\n"
          ]
        }
      ],
      "source": [
        "# # Load environment variables from a `.env` file (e.g., OPENAI_API_KEY=sk-...)\n",
        "# load_dotenv()\n",
        "\n",
        "# # Create the OpenAI client using the API key loaded from environment\n",
        "# client = openai.OpenAI()\n",
        "\n",
        "# # Folder where the MP3 files are stored\n",
        "# AUDIO_FOLDER = \"downloads\"\n",
        "\n",
        "# # Path to the cache file that stores transcripts to avoid reprocessing\n",
        "# CACHE_FILE = \"transcripts_api.json\"\n",
        "\n",
        "# # List of file numbers we want to transcribe (e.g., 21144-01.mp3, 21144-04.mp3, etc.)\n",
        "# FILE_NUMBERS = [1, 4, 5, 6, 14, 22, 24, 25, 26, 27]\n",
        "\n",
        "# # Load the transcript cache if it already exists, otherwise start with an empty dict\n",
        "# if os.path.exists(CACHE_FILE):\n",
        "#     with open(CACHE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "#         transcript_cache = json.load(f)\n",
        "# else:\n",
        "#     transcript_cache = {}\n",
        "\n",
        "# # Function to transcribe a single MP3 file using the OpenAI Whisper-1 API\n",
        "# def transcribe_with_openai(mp3_path):\n",
        "#     # If the transcription is already cached, return it to avoid extra API cost\n",
        "#     if mp3_path in transcript_cache:\n",
        "#         print(f\"Transcript found in cache: {mp3_path}\")\n",
        "#         return transcript_cache[mp3_path]\n",
        "\n",
        "#     print(f\"Transcribing with Whisper-1: {mp3_path}\")\n",
        "#     try:\n",
        "#         # Open the audio file in binary mode\n",
        "#         with open(mp3_path, \"rb\") as audio_file:\n",
        "#             # Send the file to OpenAI's Whisper-1 transcription API\n",
        "#             transcript = client.audio.transcriptions.create(\n",
        "#                 model=\"whisper-1\",\n",
        "#                 file=audio_file\n",
        "#             )\n",
        "#             # Extract the transcript text from the API response\n",
        "#             text = transcript.text\n",
        "\n",
        "#             # Save the result in the cache\n",
        "#             transcript_cache[mp3_path] = text\n",
        "\n",
        "#             # Write the updated cache back to the JSON file\n",
        "#             with open(CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "#                 json.dump(transcript_cache, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "#             return text\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error transcribing {mp3_path}: {e}\")\n",
        "#         return None\n",
        "\n",
        "# # Main loop: go through each file number and transcribe the corresponding MP3\n",
        "# for num in FILE_NUMBERS:\n",
        "#     file_name = f\"21144-{num:02d}.mp3\"  # Format file name with leading zero (e.g., 01, 04, etc.)\n",
        "#     file_path = os.path.join(AUDIO_FOLDER, file_name)  # Full path to the file\n",
        "\n",
        "#     # Skip files that don't exist\n",
        "#     if not os.path.exists(file_path):\n",
        "#         print(f\"File not found: {file_path}\")\n",
        "#         continue\n",
        "\n",
        "#     # Transcribe the file and print a short summary\n",
        "#     result_text = transcribe_with_openai(file_path)\n",
        "#     if result_text:\n",
        "#         print(f\"Transcribed ({file_name}): {len(result_text.split())} palabras\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Script para usar OpenAI API o HuggingFace\n",
        "#### Para usar OPEN AI API:\n",
        "- **MODEL_SOURCE = \"openai\"**\n",
        "\n",
        "#### Para usar HuggingFace:\n",
        "- **MODEL_SOURCE = \"huggingface\"**\n",
        "y también seleccionar el model id que se va a usar:\n",
        "- **HF_MODEL_ID = \"clu-ling/whisper-medium-spanish\"** (puedes cambiarlo por otro modelo de HuggingFace)\n",
        "\n",
        "### NOTA: Si se usa OpenAI API, se debe tener la variable de entorno `OPENAI_API_KEY` configurada con la clave de API de OpenAI.\n",
        "\n",
        "### Si el model source es **MODEL_SOURCE = \"openai\"** entonces no importa el valor de `HF_MODEL_ID`, pero si el model source es **MODEL_SOURCE = \"huggingface\"** entonces se debe especificar un modelo válido de HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎙️ Whisper-1 API: downloads/21144-01.mp3\n",
            "✅ Transcribed: 21144-01.mp3 (103 palabras)\n",
            "🎙️ Whisper-1 API: downloads/21144-04.mp3\n",
            "✅ Transcribed: 21144-04.mp3 (142 palabras)\n",
            "🎙️ Whisper-1 API: downloads/21144-05.mp3\n",
            "✅ Transcribed: 21144-05.mp3 (135 palabras)\n",
            "🎙️ Whisper-1 API: downloads/21144-06.mp3\n",
            "✅ Transcribed: 21144-06.mp3 (152 palabras)\n",
            "🎙️ Whisper-1 API: downloads/21144-14.mp3\n",
            "✅ Transcribed: 21144-14.mp3 (103 palabras)\n",
            "🎙️ Whisper-1 API: downloads/21144-22.mp3\n",
            "✅ Transcribed: 21144-22.mp3 (103 palabras)\n",
            "🎙️ Whisper-1 API: downloads/21144-24.mp3\n",
            "✅ Transcribed: 21144-24.mp3 (140 palabras)\n",
            "🎙️ Whisper-1 API: downloads/21144-25.mp3\n",
            "✅ Transcribed: 21144-25.mp3 (92 palabras)\n",
            "🎙️ Whisper-1 API: downloads/21144-26.mp3\n",
            "✅ Transcribed: 21144-26.mp3 (118 palabras)\n",
            "🎙️ Whisper-1 API: downloads/21144-27.mp3\n",
            "✅ Transcribed: 21144-27.mp3 (95 palabras)\n"
          ]
        }
      ],
      "source": [
        "# Option: \"openai\" or \"huggingface\"\n",
        "MODEL_SOURCE = \"openai\"  # or \"huggingface\"\n",
        "HF_MODEL_ID = \"clu-ling/whisper-medium-spanish\"\n",
        "\n",
        "# Config\n",
        "AUDIO_FOLDER = \"downloads\"\n",
        "FILE_NUMBERS = [1, 4, 5, 6, 14, 22, 24, 25, 26, 27]\n",
        "CACHE_FILE = f\"transcripts_{MODEL_SOURCE}.json\"\n",
        "\n",
        "# Load cache\n",
        "if os.path.exists(CACHE_FILE):\n",
        "    with open(CACHE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        transcript_cache = json.load(f)\n",
        "else:\n",
        "    transcript_cache = {}\n",
        "\n",
        "# ----- OPENAI SETUP -----\n",
        "if MODEL_SOURCE == \"openai\":\n",
        "    import openai\n",
        "    load_dotenv()\n",
        "    client = openai.OpenAI()\n",
        "\n",
        "    def transcribe(mp3_path):\n",
        "        if mp3_path in transcript_cache:\n",
        "            print(f\"🗃️ Cache: {mp3_path}\")\n",
        "            return transcript_cache[mp3_path]\n",
        "        print(f\"🎙️ Whisper-1 API: {mp3_path}\")\n",
        "        try:\n",
        "            with open(mp3_path, \"rb\") as audio_file:\n",
        "                response = client.audio.transcriptions.create(\n",
        "                    model=\"whisper-1\",\n",
        "                    file=audio_file\n",
        "                )\n",
        "                text = response.text\n",
        "                transcript_cache[mp3_path] = text\n",
        "                with open(CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(transcript_cache, f, ensure_ascii=False, indent=2)\n",
        "                return text\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")\n",
        "            return None\n",
        "\n",
        "# ----- HUGGING FACE SETUP -----\n",
        "elif MODEL_SOURCE == \"huggingface\":\n",
        "    from transformers import pipeline\n",
        "    import torch\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"automatic-speech-recognition\",\n",
        "        model=HF_MODEL_ID,\n",
        "        chunk_length_s=30,\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "\n",
        "    def transcribe(mp3_path):\n",
        "        if mp3_path in transcript_cache:\n",
        "            print(f\"🗃️ Cache: {mp3_path}\")\n",
        "            return transcript_cache[mp3_path]\n",
        "        print(f\"🎙️ Hugging Face: {mp3_path}\")\n",
        "        try:\n",
        "            result = pipe(mp3_path)\n",
        "            text = result[\"text\"]\n",
        "            transcript_cache[mp3_path] = text\n",
        "            with open(CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(transcript_cache, f, ensure_ascii=False, indent=2)\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")\n",
        "            return None\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"Invalid MODEL_SOURCE. Use 'openai' or 'huggingface'.\")\n",
        "\n",
        "# ----- Main loop -----\n",
        "for num in FILE_NUMBERS:\n",
        "    file_name = f\"21144-{num:02d}.mp3\"\n",
        "    file_path = os.path.join(AUDIO_FOLDER, file_name)\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"⚠️ Not found: {file_path}\")\n",
        "        continue\n",
        "\n",
        "    result = transcribe(file_path)\n",
        "    if result:\n",
        "        print(f\"✅ Transcribed: {file_name} ({len(result.split())} palabras)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'downloads/21144-01.mp3': 'Las fábulas de Sopo Grabado para LibriVox.org por Paulino www.paulino.info Fábula número 61 El lobo y el cordero en el templo Dándose cuenta de que era perseguido por un lobo, un pequeño corderito decidió refugiarse en un templo cercano. Lo llamó lobo y le dijo que si el sacrificador lo encontraba allí adentro, lo inmolaría a su dios. Mejor así, replicó el cordero, prefiero ser víctima para un dios a tener que perecer en tus colmillos. Si sin remedio vamos a ser sacrificados, más nos vale que sea con el mayor honor. Fin de la fábula Esta es una grabación del dominio público.',\n",
              " 'downloads/21144-04.mp3': 'Las fábulas de Esopo, grabado para LibriVox.org por Roberto Antonio Muñoz, fábula número 64, El Lobo y la Cruz. A un lobo que comía un hueso, se le atragantó el hueso en la garganta y corría por todas partes en busca de auxilio. Encontró en su correra a una grulla y le pidió que le salvara de aquella situación y que enseguida le pagaría por ello. Aceptó la grulla e introdujo su cabeza en la boca del lobo, sacando de la garganta el hueso atravesado. Pidió entonces la cancelación de la paga convenida. Oye, amiga, dijo el lobo, ¿no crees que es suficiente paga con haber sacado tu cabeza sana y salva de mi boca? Nunca hagas favores a malvados, traficantes o corruptos, pues mucha paga tendrías si te dejan sano y salvo. Fin de fábula. Esta grabación es de dominio público.',\n",
              " 'downloads/21144-05.mp3': 'Las fábulas de Sopo. Grabado para LibriVox.org por Karen Savage. Fábula número 65. El lobo y el caballo. Pasaba un lobo por un sembrado de cebada, pero como no era comida de su gusto, la dejó y siguió su camino. Encontró al rato a un caballo y le llevó al campo comentándole la gran cantidad de cebada que había hallado, pero que en vez de comérsela a él, mejor se la había dejado porque le agradaba más oír el ruido de sus dientes al masticarla. Pero el caballo le repuso. Amigo, si los lobos comieran cebada, no hubieras preferido complacer a tus oídos sino a tu estómago. A todo malvado, aunque parezca actuar como bueno, no debe de creérsele. Fin de fábula. Esta grabación es de dominio público. Subtítulos realizados por la comunidad de Amara.org',\n",
              " 'downloads/21144-06.mp3': 'Las fábulas de Esopo, grabado para LibriVox.org por Alejandro González Calderón. Fábula número 66. El Lobo y el Asno. Un lobo fue elegido rey entre sus congéneres y decretó una ley ordenando que lo que cada uno capturase en la casa, lo pusiera en común y lo repartiese por partes iguales entre todos. De esta manera ya no tendrían los lobos que devorarse unos a otros en épocas de hambre. Pero en eso le escuchó un asno, que estaba por ahí cerca, y moviendo sus orejas le dijo ¡Magnífica idea ha brotado de tu corazón! Pero, ¿por qué has escondido todo tu botín en tu cueva? Llévalo a la comunidad y repártelo también como lo has decretado. El lobo, descubierto y confundido, derogó su ley. Si alguna vez llegas a tener poder de legislar, sé el primero en cumplir tus propias leyes. Fin de la fábula. Esta grabación es de dominio público.',\n",
              " 'downloads/21144-14.mp3': 'Las fábulas de Sopo. Grabado para LibriVox.org por El Ochito. Fábula número 74. El Lobo y el Cabrito Encerrado. Protegido por la seguridad del corral de una casa, un cabrito vio pasar a un lobo y comenzó a insultarle burlándose ampliamente de él. El lobo serenamente le replicó. ¡Infeliz! Sé que no eres tú quien me está insultando, sino el sitio en que te encuentras. Muy a menudo no es el valor, sino la ocasión y el lugar quienes proveen el enfrentamiento arrogante ante los poderosos. Fin de la fábula. Esta grabación es del dominio público. Subtítulos realizados por la comunidad de Amara.org',\n",
              " 'downloads/21144-22.mp3': 'Las fábulas de Esopo. Grabado para LibriVox.org por El Ochito. Fábula número 82. El perro y la almeja. Un perro de esos, acostumbrados a comer huevos, al ver una almeja, no lo pensó dos veces, y creyendo que se trataba de un huevo, se la tragó inmediatamente. Desgarradas luego sus entrañas, se sintió muy mal, y se dijo, bien merecido lo tengo, por creer que todo lo que veo redondo son huevos. Nunca tomes un asunto sin antes reflexionar, para no entrar luego en extrañas dificultades. Fin de la fábula. Esta grabación es del dominio público. Subtítulos realizados por la comunidad de Amara.org',\n",
              " 'downloads/21144-24.mp3': 'Las fábulas de Sopo. Grabado para LibriVox.org por Karen Savich. Fábula número 84. El perro y el reflejo en el río. Badeaba un perro un río, llevando en su hocico un sabroso pedazo de carne. Vio su propio reflejo en el agua del río y creyó que aquel reflejo era, en realidad, otro perro que llevaba un trozo de carne mayor que el suyo. Y deseando adueñarse del pedazo ajeno, soltó el suyo para arrebatar el trozo a su supuesto compadre. Pero el resultado fue que se quedó sin el propio y sin el ajeno. Este porque no existía, solo era un reflejo, y el otro, el verdadero, porque se lo llevó a la corriente. Nunca codices el bien ajeno, pues puedes perder lo que ya has adquirido con tu esfuerzo. Fin de fábula. Esta grabación es de dominio público.',\n",
              " 'downloads/21144-25.mp3': 'Las fábulas de Sopo, grabado para Librevox.org Fábula número 85, El perro y el carnicero Penetró un perro en una carnicería, y notando que el carnicero estaba muy ocupado con sus clientes, cogió un trozo de carne y salió corriendo. Se volvió el carnicero, y viéndole huir, y sin poder hacer ni nada, exclamó, —¡Oye, amigo! Ahí donde te encuentre, no dejaré de mirarte. No esperes a que suceda un accidente para pensar en cómo evitarlo. Fin de fábula. Esta grabación es del dominio público. Subtítulos realizados por la comunidad de Amara.org',\n",
              " 'downloads/21144-26.mp3': 'Las fábulas de Esopo. Grabado para LibriVox.org por El Ochito. Fábula número 86. El perro con campanilla. Había un perro que acostumbraba a morder sin razón. Le puso su amo una campanilla para advertirle a la gente de su presencia cercana. Y el can, sonando la campanilla, se fue a la plaza pública a presumir. Mas una sabia perra, ya avanzada de años, le dijo. ¿De qué presumes tanto, amigo? Sé que no llevas esa campanilla por tus grandes virtudes, sino para anunciar tu maldad oculta. Los halagos que se hacen a sí mismo, los fanfarrones, sólo delatan sus mayores defectos. Fin de la fábula. Esta grabación es del dominio público. Subtítulos realizados por la comunidad de Amara.org',\n",
              " 'downloads/21144-27.mp3': 'Las fábulas de Esopo. Grabado para LibriVox.org por Elo Chito. Fábula número 87. El perro que perseguía al león. Un perro de casa se encontró con un león y partió en su persecución. Pero el león se volvió rugiendo y el perro, todo temorizado, retrocedió rápidamente por el mismo camino. Le vio una zorra y le dijo, ¡Perro infeliz! Primero perseguías al león y ya ni siquiera soportas sus surgidos. Cuando entres a una empresa, mantente siempre listo a afrontar imprevistos que no te imaginabas. Fin de la fábula. Esta grabación es del dominio público.'}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transcript_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Justificación técnica de **whisper-1**\n",
        "| Criterio                          | Valoración del modelo `whisper-1` de OpenAI                                                                                                                               |\n",
        "| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Precisión en español**          | Entrenado multilingüe sobre millones de horas, tiene una precisión excelente en español, comparable o superior a modelos fine-tuneados.                                   |\n",
        "| **Robustez ante ruidos**          | Capaz de manejar variaciones en calidad de audio, acentos, pausas, y entonación sin afectar notablemente la transcripción.                                                |\n",
        "| **Preprocesamiento mínimo**       | Genera texto limpio, con puntuación adecuada y frases coherentes, lo que reduce la necesidad de procesamiento adicional.                                                  |\n",
        "| **Uso sin infraestructura local** | No requiere GPU ni instalación de modelos grandes; todo el procesamiento ocurre en la nube de OpenAI. Ideal para laptops o estaciones sin capacidad de cómputo intensiva. |\n",
        "| **Velocidad y escalabilidad**     | Muy rápido y escalable; ideal para proyectos pequeños y también para futuras automatizaciones más amplias.                                                                |\n",
        "| **Facilidad de integración**      | Uso vía API REST con librerías bien documentadas (`openai-python`), lo que simplifica la integración en pipelines.                                                        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparación con modelos alternativos\n",
        "- **Modelos open-source (clu-ling/whisper-large-v2-spanish):** aunque preciso, requieren gran cantidad de RAM y preferentemente GPU para un rendimiento adecuado. En sistemas solo con CPU, el tiempo de procesamiento puede ser excesivo.\n",
        "\n",
        "- **Modelos pequeños (whisper-small, medium):** menos exigentes, pero pierden precisión, especialmente en español y en audios narrativos donde cada palabra tiene peso semántico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusión\n",
        "**whisper-1** es el modelo más adecuado para el caso de uso porque:\n",
        "\n",
        "- Tiene la mayor precisión posible en español sin requerir entrenamiento adicional.\n",
        "\n",
        "- Funciona perfectamente sin necesidad de GPU, ideal para un entorno local.\n",
        "\n",
        "- Genera transcripciones limpias, coherentes y con puntuación.\n",
        "\n",
        "- Minimiza la necesidad de procesamiento posterior (limpieza, normalización, etc.).\n",
        "\n",
        "- Evita complicaciones de instalación, consumo de recursos y errores locales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM0D83j8EWiN"
      },
      "source": [
        "# **Ejercicio 2b:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiFG5q88EYHU"
      },
      "source": [
        "* #### **Eliminar el inicio y final comunes de los textos extraídos de cada fábula.**\n",
        "\n",
        "* #### **Sugerencia:** Pueden guardar esta información en un archivo tipo JSON, para que al estar probando diferentes opciones en los ejercicios siguientes, puedan recuperar rápidamente la información de cada video/fábula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KkbeTmeon_RP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Limpieza completada. Archivo guardado como 'transcripts_cleaned.json'\n"
          ]
        }
      ],
      "source": [
        "# Load original transcripts\n",
        "with open(f\"transcripts_{MODEL_SOURCE}.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    transcripts = json.load(f)\n",
        "\n",
        "cleaned_transcripts = {}\n",
        "\n",
        "for path, text in transcripts.items():\n",
        "    original_text = text.strip()\n",
        "\n",
        "    # Step 1: Start from \"Fábula número X\"\n",
        "    match = re.search(r\"(F[áa]bula número \\d+)\", original_text, flags=re.IGNORECASE)\n",
        "    if match:\n",
        "        cleaned = original_text[match.start():]\n",
        "    else:\n",
        "        cleaned = original_text  # fallback if pattern not found\n",
        "\n",
        "    # Step 2: Remove from \"Fin de la fábula\" OR \"Fin de fábula\" (inclusive)\n",
        "    outro_match = re.search(r\"Fin de (la )?fábula\", cleaned, flags=re.IGNORECASE)\n",
        "    if outro_match:\n",
        "        cleaned = cleaned[:outro_match.start()].strip()\n",
        "\n",
        "    cleaned_transcripts[path] = cleaned\n",
        "\n",
        "# Save to cleaned JSON file\n",
        "with open(\"transcripts_cleaned.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(cleaned_transcripts, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Limpieza completada. Archivo guardado como 'transcripts_cleaned.json'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'downloads/21144-01.mp3': 'Fábula número 61 El lobo y el cordero en el templo Dándose cuenta de que era perseguido por un lobo, un pequeño corderito decidió refugiarse en un templo cercano. Lo llamó lobo y le dijo que si el sacrificador lo encontraba allí adentro, lo inmolaría a su dios. Mejor así, replicó el cordero, prefiero ser víctima para un dios a tener que perecer en tus colmillos. Si sin remedio vamos a ser sacrificados, más nos vale que sea con el mayor honor.',\n",
              " 'downloads/21144-04.mp3': 'fábula número 64, El Lobo y la Cruz. A un lobo que comía un hueso, se le atragantó el hueso en la garganta y corría por todas partes en busca de auxilio. Encontró en su correra a una grulla y le pidió que le salvara de aquella situación y que enseguida le pagaría por ello. Aceptó la grulla e introdujo su cabeza en la boca del lobo, sacando de la garganta el hueso atravesado. Pidió entonces la cancelación de la paga convenida. Oye, amiga, dijo el lobo, ¿no crees que es suficiente paga con haber sacado tu cabeza sana y salva de mi boca? Nunca hagas favores a malvados, traficantes o corruptos, pues mucha paga tendrías si te dejan sano y salvo.',\n",
              " 'downloads/21144-05.mp3': 'Fábula número 65. El lobo y el caballo. Pasaba un lobo por un sembrado de cebada, pero como no era comida de su gusto, la dejó y siguió su camino. Encontró al rato a un caballo y le llevó al campo comentándole la gran cantidad de cebada que había hallado, pero que en vez de comérsela a él, mejor se la había dejado porque le agradaba más oír el ruido de sus dientes al masticarla. Pero el caballo le repuso. Amigo, si los lobos comieran cebada, no hubieras preferido complacer a tus oídos sino a tu estómago. A todo malvado, aunque parezca actuar como bueno, no debe de creérsele.',\n",
              " 'downloads/21144-06.mp3': 'Fábula número 66. El Lobo y el Asno. Un lobo fue elegido rey entre sus congéneres y decretó una ley ordenando que lo que cada uno capturase en la casa, lo pusiera en común y lo repartiese por partes iguales entre todos. De esta manera ya no tendrían los lobos que devorarse unos a otros en épocas de hambre. Pero en eso le escuchó un asno, que estaba por ahí cerca, y moviendo sus orejas le dijo ¡Magnífica idea ha brotado de tu corazón! Pero, ¿por qué has escondido todo tu botín en tu cueva? Llévalo a la comunidad y repártelo también como lo has decretado. El lobo, descubierto y confundido, derogó su ley. Si alguna vez llegas a tener poder de legislar, sé el primero en cumplir tus propias leyes.',\n",
              " 'downloads/21144-14.mp3': 'Fábula número 74. El Lobo y el Cabrito Encerrado. Protegido por la seguridad del corral de una casa, un cabrito vio pasar a un lobo y comenzó a insultarle burlándose ampliamente de él. El lobo serenamente le replicó. ¡Infeliz! Sé que no eres tú quien me está insultando, sino el sitio en que te encuentras. Muy a menudo no es el valor, sino la ocasión y el lugar quienes proveen el enfrentamiento arrogante ante los poderosos.',\n",
              " 'downloads/21144-22.mp3': 'Fábula número 82. El perro y la almeja. Un perro de esos, acostumbrados a comer huevos, al ver una almeja, no lo pensó dos veces, y creyendo que se trataba de un huevo, se la tragó inmediatamente. Desgarradas luego sus entrañas, se sintió muy mal, y se dijo, bien merecido lo tengo, por creer que todo lo que veo redondo son huevos. Nunca tomes un asunto sin antes reflexionar, para no entrar luego en extrañas dificultades.',\n",
              " 'downloads/21144-24.mp3': 'Fábula número 84. El perro y el reflejo en el río. Badeaba un perro un río, llevando en su hocico un sabroso pedazo de carne. Vio su propio reflejo en el agua del río y creyó que aquel reflejo era, en realidad, otro perro que llevaba un trozo de carne mayor que el suyo. Y deseando adueñarse del pedazo ajeno, soltó el suyo para arrebatar el trozo a su supuesto compadre. Pero el resultado fue que se quedó sin el propio y sin el ajeno. Este porque no existía, solo era un reflejo, y el otro, el verdadero, porque se lo llevó a la corriente. Nunca codices el bien ajeno, pues puedes perder lo que ya has adquirido con tu esfuerzo.',\n",
              " 'downloads/21144-25.mp3': 'Fábula número 85, El perro y el carnicero Penetró un perro en una carnicería, y notando que el carnicero estaba muy ocupado con sus clientes, cogió un trozo de carne y salió corriendo. Se volvió el carnicero, y viéndole huir, y sin poder hacer ni nada, exclamó, —¡Oye, amigo! Ahí donde te encuentre, no dejaré de mirarte. No esperes a que suceda un accidente para pensar en cómo evitarlo.',\n",
              " 'downloads/21144-26.mp3': 'Fábula número 86. El perro con campanilla. Había un perro que acostumbraba a morder sin razón. Le puso su amo una campanilla para advertirle a la gente de su presencia cercana. Y el can, sonando la campanilla, se fue a la plaza pública a presumir. Mas una sabia perra, ya avanzada de años, le dijo. ¿De qué presumes tanto, amigo? Sé que no llevas esa campanilla por tus grandes virtudes, sino para anunciar tu maldad oculta. Los halagos que se hacen a sí mismo, los fanfarrones, sólo delatan sus mayores defectos.',\n",
              " 'downloads/21144-27.mp3': 'Fábula número 87. El perro que perseguía al león. Un perro de casa se encontró con un león y partió en su persecución. Pero el león se volvió rugiendo y el perro, todo temorizado, retrocedió rápidamente por el mismo camino. Le vio una zorra y le dijo, ¡Perro infeliz! Primero perseguías al león y ya ni siquiera soportas sus surgidos. Cuando entres a una empresa, mantente siempre listo a afrontar imprevistos que no te imaginabas.'}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_transcripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PKaB_Ge0Shc"
      },
      "source": [
        "# **Ejercicio 3:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNrqcQFe0VWR"
      },
      "source": [
        "* #### **Apliquen el proceso de limpieza que consideren adecuado.**\n",
        "\n",
        "* #### **Justifiquen los pasos de limpieza utilizados. Tomen en cuenta que el texto extraído de cada fábula es relativamente pequeño.**\n",
        "\n",
        "* #### **En caso de que decidan no aplicar esta etapa de limpieza, deberán justificarlo.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pqwiCCdpq8D_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenización completada. Guardado en 'transcripts_tokenized.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/eahumada/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Incluyan a continuación todas las celdas (de código o texto) que deseen...\n",
        "# Download tokenizer (if needed)\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "# Load cleaned transcripts\n",
        "with open(\"transcripts_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    cleaned = json.load(f)\n",
        "\n",
        "tokenized = {}\n",
        "\n",
        "for path, text in cleaned.items():\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation (keep accented letters and ñ)\n",
        "    text = re.sub(r\"[^\\w\\sáéíóúüñ]\", \"\", text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text, language=\"spanish\")\n",
        "\n",
        "    tokenized[path] = tokens\n",
        "\n",
        "# Save tokenized result\n",
        "with open(\"transcripts_tokenized.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(tokenized, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Tokenización completada. Guardado en 'transcripts_tokenized.json'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['fábula', 'número', '61', 'el', 'lobo', 'y', 'el', 'cordero', 'en', 'el']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized[\"downloads/21144-01.mp3\"][:10]  # Show first 10 tokens of the first file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2ywrmsMP_EF"
      },
      "source": [
        "# **Ejercicio 4:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xFpnt0A0Ub7"
      },
      "outputs": [],
      "source": [
        "# Incluyan a continuación todas las celdas (de código o texto) que deseen...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blrrs1sWwkSx"
      },
      "source": [
        "# **Ejercicio 5a y 5b:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWvzQ-aNwsVk"
      },
      "source": [
        "* #### **5a: Mediante el LLM que hayan seleccionado, generar un único enunciado que describa o resuma cada fábula.**\n",
        "\n",
        "* #### **5b: Mediante el LLM que hayan seleccionado, generar tres posibles enunciados diferentes relacionados con la historia de la fábula.**\n",
        "\n",
        "* #### **Sugerencia:** En realidad los dos incisos a y b se pueden obtener con un solo prompt que solicite la información y el formato correspondiente para cada una de estas partes. Por ejemplo, para cada fábula la salida puede ser un primer enunciado genérico que resume o describe dicha temática; seguido de tres enunciados, cada uno hablando sobre una situación o parte diferente de la fábula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9UkVPxM0Xii"
      },
      "outputs": [],
      "source": [
        "# Incluyan a continuación todas las celdas (de código o texto) que deseen...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx-dZSFJz9cK"
      },
      "source": [
        "# **Ejercicio 6:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w3usdaC0BCj"
      },
      "source": [
        "* #### **Incluyan sus conclusiones de la actividad audio-a-texto:**\n",
        "\n",
        "\n",
        "\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtB5Q3m41YQ0"
      },
      "source": [
        "# **Fin de la actividad LDA y LMM: audio-a-texto**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1BtP-Sk0DT-M",
        "6uYgtCvvJSmq",
        "NM0D83j8EWiN",
        "6PKaB_Ge0Shc",
        "i2ywrmsMP_EF",
        "Blrrs1sWwkSx",
        "Kx-dZSFJz9cK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
